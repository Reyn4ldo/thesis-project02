# Quick Reference Card

## ğŸš€ Essential Commands

### Setup (One-time)
```bash
bash setup_and_test.sh
# OR
pip install -r requirements.txt
```

### Run Everything
```bash
python run_pipeline.py          # Complete analysis (all phases)
```

### Launch Dashboard
```bash
streamlit run src/deployment/app.py
# Access: http://localhost:8501
```

### Generate Report
```bash
python generate_report.py      # Creates FINAL_REPORT.md
```

### Docker
```bash
docker-compose up -d           # Start
docker-compose logs -f         # View logs
docker-compose down           # Stop
```

## ğŸ“‚ Key Files

| File | Purpose |
|------|---------|
| `run_pipeline.py` | Execute all analyses |
| `generate_report.py` | Create comprehensive report |
| `src/deployment/app.py` | Interactive dashboard |
| `requirements.txt` | Python dependencies |
| `README.md` | Project overview |
| `USAGE_GUIDE.md` | Detailed instructions |
| `PROJECT_SUMMARY.md` | Implementation details |

## ğŸ“Š Individual Modules

```bash
# Preprocessing
python src/preprocessing/clean_data.py
python src/preprocessing/feature_engineering.py

# Classification (6 algorithms)
python src/classification/train_models.py

# Clustering (k-Means, Hierarchical, DBSCAN)
python src/clustering/cluster_analysis.py

# Association Rules (Apriori, FP-Growth)
python src/association_rules/mine_rules.py

# Visualizations (PCA, t-SNE, UMAP)
python src/dimensionality_reduction/visualize.py

# Statistics (correlations, tests)
python src/statistical_analysis/analyze.py
```

## ğŸ¯ What Each Phase Does

1. **Preprocessing**: Cleans data, engineers features, splits dataset
2. **Classification**: Trains 6 models, evaluates, selects best
3. **Clustering**: Identifies MDR clusters, creates heatmaps
4. **Association Rules**: Finds co-resistance patterns
5. **Dimensionality Reduction**: Creates 2D/3D visualizations
6. **Statistical Analysis**: Correlations, hypothesis tests

## ğŸ“ˆ Where to Find Results

```
data/results/
â”œâ”€â”€ classification_results_*.csv       # Model performance
â”œâ”€â”€ best_model_*.pkl                   # Trained models
â”œâ”€â”€ clustering/
â”‚   â”œâ”€â”€ cluster_labels.csv            # Assignments
â”‚   â””â”€â”€ *_heatmap.png                 # Visualizations
â”œâ”€â”€ association_rules/
â”‚   â””â”€â”€ *_rules.csv                   # Co-resistance rules
â”œâ”€â”€ dimensionality_reduction/
â”‚   â””â”€â”€ *.png                         # All plots
â””â”€â”€ statistical_analysis/
    â””â”€â”€ *_correlation.csv             # Correlations
```

## ğŸ” Dashboard Features

- **Data Overview**: Statistics, distributions
- **Classification**: Model comparison, feature importance
- **Clustering**: Cluster profiles, heatmaps
- **Association Rules**: Filter by confidence/support/lift
- **Visualizations**: Interactive PCA/t-SNE/UMAP
- **Statistics**: Correlation heatmaps, test results

## âš¡ Quick Troubleshooting

**Missing modules?**
```bash
pip install -r requirements.txt
```

**Port 8501 busy?**
```bash
streamlit run src/deployment/app.py --server.port 8502
```

**Out of memory?**
- Reduce sample size in analysis files
- Use smaller n_estimators in models

**Data not found?**
```bash
ls data/raw/raw_data.csv  # Should exist
```

## ğŸ“¦ Project Structure

```
thesis-project02/
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/              # Input data (582 samples)
â”‚   â”œâ”€â”€ processed/        # Cleaned data + features
â”‚   â””â”€â”€ results/          # All outputs
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ preprocessing/    # Data prep
â”‚   â”œâ”€â”€ classification/   # 6 ML models
â”‚   â”œâ”€â”€ clustering/       # 3 methods
â”‚   â”œâ”€â”€ association_rules/# 2 algorithms
â”‚   â”œâ”€â”€ dimensionality_reduction/  # 3 techniques
â”‚   â”œâ”€â”€ statistical_analysis/      # Tests
â”‚   â””â”€â”€ deployment/       # Dashboard
â”œâ”€â”€ run_pipeline.py       # Run all
â”œâ”€â”€ generate_report.py    # Create report
â””â”€â”€ setup_and_test.sh    # Setup
```

## ğŸ“ Algorithms Implemented

**Classification (6)**:
- Logistic Regression
- Random Forest
- XGBoost
- SVM
- k-Nearest Neighbors
- Neural Network (MLP)

**Clustering (3)**:
- k-Means
- Hierarchical
- DBSCAN

**Association Rules (2)**:
- Apriori
- FP-Growth

**Dimensionality Reduction (3)**:
- PCA
- t-SNE
- UMAP

## ğŸ’¡ Tips

- Run `python run_pipeline.py` first
- Use dashboard for interactive exploration
- Check `pipeline.log` for detailed logs
- Generate report after pipeline completes
- Docker for reproducible deployment

## ğŸ“ Support

1. Check `USAGE_GUIDE.md`
2. Review logs: `pipeline.log`
3. Read `README.md`
4. See `PROJECT_SUMMARY.md`

---

**Version**: 1.0  
**Status**: Production Ready  
**License**: MIT
